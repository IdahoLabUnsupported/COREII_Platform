# Copyright 2025, Battelle Energy Alliance, LLC, ALL RIGHTS RESERVED

### DATA INGESTION ###
# MinIO credentials
MINIO_ENDPOINT=
MINIO_ACCESS_KEY=
MINIO_SECRET_KEY=
BUCKET_NAME=

# Directories that store the raw data to be ingested
RAW_DATA_DIRECTORY_ARC=
RAW_DATA_DIRECTORY_EIA=
RAW_DATA_DIRECTORY_OSTI=
RAW_DATA_PATH_CYOTE_TABULAR=
RAW_DATA_DIRECTORY_CYOTE_REPORT=
RAW_DATA_DIRECTORY_CISA=
RAW_DATA_DIRECTORY_NIST=
RAW_DATA_DIRECTORY_MITRE=
RAW_DATA_PATH_EERE_FOAS=

SEED_DATA_PATH=

# Github/Gitlab mirror configuration
CISA_BASE_GIT_URL=
CISA_GIT_DOWNLOAD_URL=
CISA_PATH=
CISA_MIRROR_PRIVATE_TOKEN=

# Flags for ommitting datasets from ingestion
OMIT_EIA=
OMIT_CISA=
OMIT_ARC=
OMIT_CYOTE_REPORTS=
OMIT_CYOTE_OBSERVABLES=

### KG ###
# Root URL for machine learning API used for vector embeddings
ML_API_URL=

# Length of vector embeddings
VECTOR_LENGTH=

# Get database credentials
DB_NAME=
DB_PORT=
DB_HOST=
DB_USER=
DB_PASSWORD=

### KG API ###
KG_API_PORT=

### ML API ###
# Port for hosting the machine learning API
ML_API_PORT=

# VLLM model endpoints
VLLM_LLM_SERVICE_URL=
VLLM_EMBEDDING_SERVICE_URL=

# Max number of tokens to send to an embedding model.
# if more tokens are sent, string will be truncated until
# its length is below the token limit.
# This limit is a hard cap based on the embeddings model used.
EMBEDDING_TOKEN_LIMIT=

# Testing LLM usage with Llama3.3-70B model using AWS Bedrock.
# This setting will never be used in production on-premises.
MODEL_ID=
AWS_REGION=
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=

